\chapter*{Abstract}

Assembly of complex structures is a challenging task that requires multidisciplinary knowledge and high level of precision. In the past this was achieved by either manually defining the robot desired movements or by lead through programming. The development of new sensors and the significant increase in computation capabilities made robotic arms easier to (re)program, allowing mass customization of products. This project aims to provide an intuitive and easy to use robot assembly system, capable of learning through demonstration and cooperate with humans in complex tasks. Semantic interpretation of the operator movements will allow to extract intentional tasks based on the object's relative position and the cumulative knowledge gained during the assembly procedure. Moreover, to improve the Human Robot Interaction, an augmented reality interface using projection mapping and gesture recognition will provide a fast and intuitive way to exchange knowledge / intentions between the robot and the operator.
