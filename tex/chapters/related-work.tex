\chapter{Related work}\label{chap:related-work}

\section*{}

Human-robot interaction systems capable of learning from demonstration offer great flexibility to industrial applications in which the assembly lines have limited / customized production \cite{Patel2012}. They allow fast reprogramming of robots and allow cooperative assembly of complex and delicate tasks \cite{Sumi2009,Edsinger2007}. These systems can perform active perception of the environment \cite{Yan2014,Goodrich2007} with a wide range of sensors (\gls{tof}, RGB-D, visual, audio, tactile, laser) and a multitude of recognition algorithms such as visual / geometric feature association, template / shape matching, color / intensity / texture segmentation (among many others). Moreover, by semantic analyzing the operator movements \cite{Roitberg2014} through hand / skeletal tracking they are able to extract the skills / knowledge \cite{Nikolaidis14,Goto2013} that is required to assemble a new composite object. Besides learning new assembly skills through techniques such as \glspl{svm}, clustering, \gls{nn}, boosting, \gls{hmm}, \gls{gmm} (to name a few), these perception systems can also be used to implement communication protocols, allowing a more natural interaction (gesture, posture, voice) between the robot and the operator \cite{Gleeson2013,Calisgan2012,Haddadi2013}.


\section{Robotic manipulators}

FF


\subsection{Robotic arms}

FF


\subsection{Motion planners}

FF


Articles:\\
- Integrated Grasp and Motion Planning using Independent Contact Regions\\
- MOPL - A Multi-Modal Path Planner for Generic Manipulation Tasks\\
- Development of Manipulation Planning Algorithm for a Dual-arm Robot Assembly Task


\subsection{Robotic grippers}

FF


\subsection{Gripping algorithms}

FF


\subsection{Force control}

FF


\subsection{Automatic tool change}

FF


\subsection{Environment fixtures}

FF


\section{Perception sensors}

FF


\subsection{Image acquisition}\label{sec:image-acquisition}

FF


\subsection{Point cloud acquisition}\label{sec:point-cloud-acquisition}

Point clouds can be retrieved with a wide range of sensors with varying levels of precision and acquisition time \cite{Sansoni2009}. The next sections provide a brief overview of the main technologies capable of generating point clouds useful for 3D perception.


\subsubsection{Camera-Laser}

FF


\subsubsection{Stereo vision}

Stereo vision systems (example in \cref{fig:stereo-cameras}) can generate 3D representations of the environment by comparing the displacement of corresponding points in the two ambient images (\cref{fig:stereo-vision} gives an overview of such a system). This can be achieved because the relative position of the cameras is known. As such, points farther away will have smaller displacement between images than points closer to the cameras. In the end, a disparity image is obtained, that can then be converted to a point cloud representation of the environment.

Given that the accuracy of the disparity image relies heavily in the correct matching of points between the left and right image, some stereo vision systems employ active observation by projecting a pattern into the environment in order to refine the point matching (example of hardware setup in \cref{fig:pr2-active-stereo}). This can significantly improve the accuracy if the environment has a lot of smooth surfaces with homogeneous colors.

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.21\textheight]{sensors/stereo-cameras}}
		{\caption[Stereo vision system]{Stereo vision system \cite{Kaczurba2013}}\label{fig:stereo-cameras}}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.21\textheight]{sensors/pr2-active-stereo}}
		{\caption[PR2 head capable of active stereo vision]{PR2 head capable of active stereo vision\protect\footnotemark}\label{fig:pr2-active-stereo}}
	\end{floatrow}
\end{figure}
\footnotetext{\url{https://www.willowgarage.com/pages/pr2/overview}}


\begin{figure}[H]
	\centering
	\includegraphics[height=.31\textheight]{sensors/stereo-vision}
	\caption[Stereo vision overview]{Stereo vision overview \cite{Yang2014}}
	\label{fig:stereo-vision}
\end{figure}


\subsubsection{Structured light methods}

Structured light methods can retrieve 3D geometry from images by projecting a known pattern into the environment and analyzing its deformation (examples in \Cref{fig:structured-light,fig:kinect1-ir}). They can achieve sample rates of 30 Hz and besides 3D geometry they can also retrieve color information.


\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.21\textheight]{sensors/structured-light}}
		{\caption[Structured light system diagram]{Structured light system diagram\protect\footnotemark}\label{fig:structured-light}}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.21\textheight]{sensors/kinect1-ir}}
		{\caption[Kinect 1 \glsentrytext{ir} pattern]{Kinect 1 \glsentrytext{ir} pattern\protect\footnotemark}\label{fig:kinect1-ir}}
	\end{floatrow}
\end{figure}
\footnotetext[\the\numexpr\value{footnote}-1\relax]{\url{http://www.laserfocusworld.com/articles/2011/01/lasers-bring-gesture-recognition-to-the-home.html}}
\footnotetext[\value{footnote}]{\url{https://jahya.net/blog/how-depth-sensor-works-in-5-minutes/}}


The Kinect sensor seen in \cref{fig:kinect1} is an example of a structured light system that can achieve measurements with millimeter accuracy for objects close to the sensor. Another similar sensor is the Occipital Structure IO\footnote{\url{http://structure.io/}} which is intended for mobile devices and can be seen in \cref{fig:structure-io}.


\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[1.1\FBwidth]
		{\includegraphics[height=.15\textheight]{sensors/kinect1}}
		{\caption[Kinect 2 sensor]{Kinect sensor\protect\footnotemark}\label{fig:kinect1}}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.15\textheight]{sensors/structure-io}}
		{\caption[Structure IO sensor]{Structure IO sensor\protect\footnotemark}\label{fig:structure-io}}
	\end{floatrow}
\end{figure}
\footnotetext[\the\numexpr\value{footnote}-1\relax]{\url{https://msdn.microsoft.com/en-us/library/jj131033.aspx}}
\footnotetext[\value{footnote}]{\url{http://structure.io/press}}


\subsubsection{\glsentrydesc{tof} methods}\label{sec:tof-methods}

\gls{tof} or \gls{toa} methods can be used to calculate distances based on the amount of time that a given wave takes from the moment it is created to the moment it is received (system operation overview in \cref{fig:time-of-flight}). By acquiring a large amount of sensor readings a 3D representation of the environment can be achieved.

Since these systems rely on active interaction with the environment, they can be used without being significantly affected by lighting interferences. Nevertheless, it should be taken in consideration the conditions in which the waves propagate and also the geometry of the environment, because it can affect the path that the waves take, and as a result, lead to the decrease of precision in the measurements.


\paragraph{\glsentrytext{tof} cameras}

\gls{tof} cameras (examples in \cref{fig:mesa-sr4000,fig:kinect2}) can acquire 3D measurements of the environment with very high frame rate (30 Hz or even higher) allowing perception and mapping of the environment with very low latency, which can be a critical requirement in perception tasks that must react very fast to changes in their surroundings.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{sensors/time-of-flight}
	\caption[\glsentrydesc{tof} system diagram]{\glsentrydesc{tof} system diagram\protect\footnotemark}
	\label{fig:time-of-flight}
\end{figure}
\footnotetext{\url{http://www.laserfocusworld.com/articles/2011/01/lasers-bring-gesture-recognition-to-the-home.html}}

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[1.05\FBwidth]
		{\includegraphics[height=.16\textheight]{sensors/mesa-sr4000}}
		{\caption[Mesa SR4000 sensor]{Mesa SR4000 sensor\protect\footnotemark}\label{fig:mesa-sr4000}}
		\ffigbox[1.05\FBwidth]
		{\includegraphics[height=.16\textheight]{sensors/kinect2}}
		{\caption[Kinect 2 sensor]{Kinect 2 sensor\protect\footnotemark}\label{fig:kinect2}}
	\end{floatrow}
\end{figure}
\footnotetext[\the\numexpr\value{footnote}-1\relax]{\url{http://www.mesa-imaging.ch/products/product-overview/}}
\footnotetext[\value{footnote}]{\url{https://www.ifixit.com/Teardown/Xbox+One+Kinect+Teardown/19725}}


\paragraph{\glsentrytext{lidar}}

Light waves generated with lasers can estimate distances with millimeter accuracy at long ranges and their sensors usually have a low sample rate (below 20 Hz). Systems like \gls{lidar} (3D sensor example shown in \cref{fig:velodyne-hdl-64e}) take advantage of this fact and can be used to obtain a very detailed 3D point cloud of the environment (like the one showed in \cref{fig:lidar-scan}). Moreover, some \glspl{lidar} can also capture the environment reflectivity / intensity besides their 3D geometry.

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.21\textheight]{sensors/lidar-scan-car}}
		{\caption[\glsentrytext{lidar} scan]{\glsentrytext{lidar} scan\protect\footnotemark}\label{fig:lidar-scan}}
		\ffigbox[]
		{\includegraphics[height=.21\textheight]{sensors/velodyne-hdl-64e}}
		{\caption[3D Velodyne HDL-64E]{3D Velodyne HDL-64E\protect\footnotemark}\label{fig:velodyne-hdl-64e}}
	\end{floatrow}
\end{figure}
\footnotetext[\the\numexpr\value{footnote}-1\relax]{\url{http://www.popsci.com/cars/article/2013-09/google-self-driving-car}}
\footnotetext[\value{footnote}]{\url{http://www.velodynelidar.com/lidar/hdlproducts/hdl64e.aspx}}


\section{Perception systems}

FF

\subsection{2D computer vision}

FF


\subsection{3D perception}

FF

Articles:\\
- 3DNet - Large-Scale Object Class Recognition from CAD Models\\
- 3D Visual Perception System for Bin Picking in Automotive Sub-Assembly Automation


\section{Knowledge management}

FF

\subsection{Ontologies}

FF

Articles:\\
- An ontology for CAD data and geometric constraints as a link between product models and semantic robot task descriptions


\subsection{Skills}

FF

Articles:\\
- Definition of Hardware-Independent Robot Skills for Industrial Robotic Co-workers\\
- A Skill-Based System for Object Perception and Manipulation for Automating Kitting Tasks


\subsection{Cloud robotics}

FF

Articles:\\
- On Distributed Knowledge Bases for Robotized Small-Batch Assembly\\
- Knowledge-based Specification of Robot Motions\\
- KnowRob - A Knowledge Processing Infrastructure for cognition-enabled robots\\
- OPEN-EASE - A Knowledge Processing Service for Robots and Robotics - AI Researchers\\
- Representation and exchange of knowledge about actions, objects, and environments in the roboearth framework\\
- RoboBrain - Large-Scale Knwoledge Engine for Robots\\
- CRAM - A Cognitive Robot Abstract Machine for Everyday Manipulation in Human Environments\\
- Rapyuta - The RoboEarth Cloud Engine


\section{Leaning algorithms}

FF

Articles:\\
- Data Mining - Practical Machine Learning Tools and Techniques (3rd Ed)


\section{Teaching systems}

FF

Articles:\\
- Toward Efficient Robot Teach-In and Semantic Process Descriptions for Small Lot Sizes\\
- Efficient Model Learning from Joint-Action Demonstrations for Human-Robot Collaborative Tasks


\section{Human Machine Interface systems}

FF


\subsection{Augmented reality}

FF


\subsection{Projection mapping}

FF


\subsection{Gesture recognition}

FF

Articles:\\
- Human Activity Recognition in the Context of Industrial Human-Robot Interaction


\subsection{Natural language processing}

FF

Articles:\\
- Connecting natural language to task demonstrations and low-level control of industrial robots\\
- Describing constraint-based assembly tasks in unstructured natural language


\subsection{Human-robot cooperation}

FF

Articles:\\
- Coordination Mechanisms in Human-Robot Collaboration\\
- Human-Robot Interaction for Cooperative Manipulation - Handing Objects to One Another\\
- Identifying Nonverbal Cues for Automated Human-Robot Turn-taking\\
- Information Support Development with Human-Centered Approach for Human-Robot Collaboration in Cellular Manufacturing

\section{Assembly systems}

FF

Articles:\\
- A system for automatic planning, evaluation and execution of assembly sequences for industrial robots\\
- Error-tolerant execution of complex robot tasks based on skill primitives\\
- Efficient assembly sequence planning using stereographical projections of c-space obstacles\\
+ Assembly Planning and Task Planning - Two Prerequisites for Automated Robot Programming\\
- A new skill based robot programming language using uml-p statecharts\\
- Modeling Reusable, Platform-Independent Robot Assembly Processes\\
- Flexible assembly through integrated assembly sequence planning and grasp planning



\section{Software frameworks}

Cooperative assembly of objects by robots is a multidisciplinary problem that requires advanced computer software systems. In order to speed up the implementation and deployment of the assembly system, several frameworks and libraries will be used. Among the most important are \gls{ros} for the system architecture, \gls{pcl} for the point cloud processing and Gazebo for simulation and testing.

\subsection{\glsentrytext{ros}}

\begin{wrapfigure}{r}{0.25\textwidth}
	\centering
	\includegraphics*[width=0.88\textwidth]{software/ros-logo}
	\caption{\glsentrytext{ros} logo}
	\label{fig:ros-logo}
\end{wrapfigure}

\gls{ros}\footnote{\url{http://www.ros.org/}} \cite{Quigley2009} is a software framework designed to ease the development of robot systems. It provides seamless integration between hardware drivers and software modules, allowing a fast transition between simulation and deployment.

It's an open source project that offers a distributed computing framework with several core libraries and development tools that aims to speedup software prototyping, testing and deployment.


\subsubsection{Architecture}

The \gls{ros} architecture was designed from the beginning to be a distributed peer-to-peer software framework that could be deployed in several operating systems and implemented in a range of different programming languages. However, given that most of the \gls{ros} community prefers open source software, the Ubuntu\footnote{\url{http://www.ubuntu.com/}} operating system is the main developing and testing environment, and as such, the recommended choice for \gls{ros} developers. Moreover, considering that robotics research requires software with both performance and maintainability at its core, the C++\footnote{\url{http://www.cplusplus.com/}} programming language is used in most of the available packages, along with Python\footnote{\url{https://www.python.org/}} and Java\footnote{\url{https://www.java.com}}.

Being a distributed computing framework, \gls{ros} relies in network connections and exchange of messages to perform the intended tasks. As such, its architecture was developed to follow a publish / subscribe pattern (\gls{ros} topics\footnote{\url{http://wiki.ros.org/Topics}}) and request and reply communication paradigm (\gls{ros} services\footnote{\url{http://wiki.ros.org/Services}} and actions\footnote{\url{http://wiki.ros.org/actionlib}}). This allows \gls{ros} nodes\footnote{\url{http://wiki.ros.org/Nodes}} (operating system processes) to be deployed in different computing platforms with ease and simplifies testing and exchange of software modules.

The next sections provide a more detailed description of the main \gls{ros} architecture concepts.


\paragraph{Nodes}

\gls{ros} nodes are operating system processes that are part of the peer-to-peer communication graph. They are the fundamental building blocks of any \gls{ros} system and can be spread among several computing platforms.

In order to manage the communications between nodes, the \gls{ros} framework provides a master node (roscore\footnote{\url{http://wiki.ros.org/roscore}}) that uses \gls{xmlrpc} to maintain a communication graph of the system. This allows nodes to be started without knowing the location (\gls{ip} address and port) of the other nodes in the network and greatly simplifies their integration and exchange. Also, by using \gls{ros} launch files\footnote{\url{http://wiki.ros.org/roslaunch}} (\gls{xml} configuration files), the specification of a system communication data flow and configuration can be easily altered.

Besides handling communications, the master node (roscore) also manages system configurations through the parameter server. This allows nodes to share and change the system configuration at runtime. However, given that the parameter server is usually queried only when a node starts up, the dynamic reconfigure\footnote{\url{http://wiki.ros.org/dynamic_reconfigure}} \gls{api} can be used instead, if it is necessary to change the configuration of a node when it is already running. This is achieved by providing a callback that is asynchronously called when a configuration change is requested.

The flexibility provided by \gls{ros} in both module integration and exchange can greatly speedup testing and deployment and the possibility of changing the configuration of the system at runtime and restart software modules individually (nodes) is very useful when implementing system supervisors and recovery behaviors.


\paragraph{Nodelets}

A nodelet\footnote{\url{http://wiki.ros.org/nodelet}} is a special kind of node that aims to reduce the overhead of message exchange. This overhead can be significant when messages are very large, such as point clouds or video. To mitigate this problem, the nodelets exchange pointers to shared memory regions, instead of sending the entire messages between nodes.

To achieve this overhead reduction, some architecture changes are required. The most important being the use of threads instead of processes, and the creation of a superclass that all nodelets must inherit. This leads to the creation of plugin libraries for each nodelet instead of an executable for each node.

Another important change is the introduction of nodelet managers to allow loading and setup of nodelets in different threads inside the same process.

In terms of implementation, the transition from nodes to nodelets requires few code changes and can lead to a significant improvement of the overall system performance.


\paragraph{Topics}

\gls{ros} topics are named communication buses that follow the publish / subscribe pattern. They provide a simple method for exchanging messages between nodes and allow the decoupling of information production and consumption. This is useful when there are multiple sources of the same information or there are multiple consumers that are interested in processing the same data for different purposes. Moreover, this communication architecture allows to log and replay the exchanged messages, which can be helpful to test different algorithms with the same data.

Currently, topics can use either the \gls{tcp} or \gls{udp} protocols to exchange messages. The \gls{tcp} implementation is used by default and creates a bidirectional channel between each producer and subscriber while guaranteeing the delivery of all messages. The \gls{udp} implementation uses an unreliable and stateless transport approach, in which a subscriber listens to a given broadcast address, and has no guarantee that will receive all messages. As such, \gls{tcp} should be used when all messages must be processed, and \gls{udp} should be considered when the latency and the \gls{tcp} overhead are important issues.


\paragraph{Services}

\gls{ros} services are named communication buses that follow the request and reply protocol, in which a node asks for a given service and receives a response according to the data that was sent in the request message and the state of the service node. They are useful to query other nodes state or to request the execution of some behavior / action.


\paragraph{Actions}

\gls{ros} actions are a special kind of service in which the progress of the request can be queried. They are very useful when the request might take a long time, and gives the caller the necessary information to supervise the execution of the request and if necessary, terminate its execution.


\subsubsection{Build system}

The latest \gls{ros} build system is named catkin\footnote{\url{http://wiki.ros.org/catkin}}, and is the successor of the original rosbuild\footnote{\url{http://wiki.ros.org/rosbuild}} system. It combines CMake\footnote{\url{http://www.cmake.org/}} macros and Python scripts to allow building multiple dependent projects at the same time. It is a cross-platform build system, organized in packages and meta-packages (group of packages). Each package is a software module that can produce libraries or binaries from source code.

Catkin was designed to deal with complex build configurations, which in the case of \gls{ros} packages involves a considerable amount of build dependencies for each project. As such, catkin provides a build system that can easily find, build and link both \gls{ros} and system dependencies. Moreover, it provides install targets to allow faster code releases and simplifies builds from source for the final users.

Other useful features of the catkin build system are the concepts of workspace and overlays. Catkin uses a workspace with out-of-source builds to keep the source code separate from the build files. This allows the code directory structure to be clean of compiler generated files that are platform dependent. Moreover, it simplifies the concurrent usage of packages (overlay), because catkin gives priority to workspace packages (in relation to system packages). This is particularly useful when it is necessary to modify and test some package that has been released and is installed in the system (without having to uninstall the stable release of that package).

Finally, catkin is a cross-platform build system that can be used to build other projects that use CMake and are not related to \gls{ros}.


\subsubsection{Development tools}

\gls{ros} provides several development tools that allow introspection and visualization of the system state. They are very useful for testing, debugging and profiling.

The next sections give an overview of the most important \gls{ros} tools that are currently available.


\paragraph{Graphical User Interface tools}

The \gls{ros} development tools that have graphical user interfaces are aggregated in the rqt framework\footnote{\url{http://wiki.ros.org/rqt}}, and are loaded as plugins at runtime (some can be started as standalone applications).

Currently there is plugins to visualize sensor data (rviz\footnote{\url{http://wiki.ros.org/rviz}}); introspect the contents of topics; log and replay \gls{ros} messages (rosbag\footnote{\url{http://wiki.ros.org/rosbag}}); display node, package and coordinate systems graphs; list and filter debug messages; change configuration of running nodes (dynamic reconfigure); monitor nodes memory and processor usage and much more.


\paragraph{Command line tools}

The \gls{ros} command line tools\footnote{\url{http://wiki.ros.org/ROS/CommandLineTools}} are split across several executables and can be used for advanced introspection (rosnode, rostopic and rosservice), system configuration (rosparam), package building and management (catkin, rosdep and rosinstall) and also to search \gls{ros} message types documentation (rosmsg and rossrv).

Finally, it is available a diagnostics tool (roswtf), that can detect packages / dependencies issues and configuration problems.



\subsection{\glsentrytext{pcl}}

\begin{wrapfigure}{r}{0.25\textwidth}
	\centering
	\includegraphics*[width=0.88\textwidth]{software/pcl-logo}
	\caption{\glsentrytext{pcl} logo}
	\label{pcl-logo}
\end{wrapfigure}

The \gls{pcl}\footnote{\url{http://pointclouds.org/}} \cite{Rusu2011} is an open source project that provides algorithms for processing point clouds. These algorithms can be used to filter and register point clouds as well as perform object segmentation, recognition and tracking.

\Cref{fig:pcl-dependency-graph} gives an overview of the main modules currently available in \gls{pcl}.

\begin{figure}[H]
	\centering
	\includegraphics[width=.5\textwidth]{software/pcl-dependency-graph}
	\caption[\glsentrydesc{pcl}]{\glsentrydesc{pcl}\protect\footnotemark}
	\label{fig:pcl-dependency-graph}
\end{figure}
\footnotetext{\url{http://pointclouds.org/about/}}


\subsection{Gazebo}

\begin{wrapfigure}{r}{0.25\textwidth}
	\centering
	\includegraphics*[width=0.88\textwidth]{software/gazebo-logo}
	\caption{Gazebo logo}
	\label{fig:gazebo-logo}
\end{wrapfigure}


Gazebo\footnote{\url{http://gazebosim.org/}} is a 3D multi-robot simulator capable of generating hardware sensor data for different kinds of robots while providing a realistic environment with physics simulation and 3D visualization. It is very useful to speedup testing of algorithms with different types of robots and environments.

\Cref{fig:gazebo-ros-integration} shows how Gazebo can be used instead of a real robot, without requiring any implementation code modification (because it implements the same \gls{ros} interfaces that the hardware drivers use).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{software/gazebo-ros-integration}
	\caption[Integration of \glsentrytext{ros} and Gazebo]{Integration of \glsentrytext{ros} and Gazebo\protect\footnotemark}
	\label{fig:gazebo-ros-integration}
\end{figure}
\footnotetext{\url{http://gazebosim.org/tutorials?tut=ros_control}}



\section{Main limitations of current approaches}

FF


\section{Related research projects}

\begin{multicols}{4}
	\begin{enumerate}
		\item RoboEarth
		\item KnowRob
		\item RoboHow
		\item ROSETTA
		\item Saphari
		\item JAHIR
		\item Charm
		\item PISA
		\item James
		\item LIAA
		\item SHRINE
		\item SARAFun
		\item CogIMon
		\item CHRIS
		\item Rapyuta
		\item DAvinci
		\item C2TAM
		\item ROS-Industrial
		\item SME-Robot
		\item SME-Robotics
		\item TAPAS
		\item ACat
	\end{enumerate}
\end{multicols}

\section{Related research groups}

FF


\section{Main conferences}

FF


\section{Main journals}

FF
