\chapter{Related work}\label{chap:related-work}

\section*{}

Automation of complex assembly tasks have revolutionized the manufacturing of products for a long time. However, these systems are usually programmed for very specific tasks and require expensive reprogramming in order to perform different assembly operations. With the arrival of more cooperative and flexible robotic arm designs, this industrial trend is changing and a significant amount of tasks that are currently being performed by humans due to its assembly complexity or cognitive requirements are now being transitioned to hybrid cooperative systems [1] and in the future they might even become fully automated. These promising industrial applications lead to the creation of several research projects such as Saphari, Rosetta, JAHIR, Charm, RoboEarth, KnowRob and ROS-Industrial (among others).

Human-robot interaction systems capable of learning from demonstration offer great flexibility to industrial applications in which the assembly lines have limited / customized production [2]. They allow fast reprogramming of robots and allow cooperative assembly of complex and delicate tasks [3,4]. These systems can perform active perception of the environment [5,6] with a wide range of sensors (ToF, RGB-D, visual, audio, tactile, laser) and a multitude of recognition algorithms such as visual / geometric feature association, template / shape matching, color / intensity / texture segmentation (among many others). Moreover, by semantic analyzing the operator movements [7] through hand / skeletal tracking they are able to extract the skills / knowledge [8,9] that is required to assemble a new composite object. Besides learning new assembly skills through techniques such as Support Vector Machines, clustering, neural networks, boosting, Hidden Markov Models, Gaussian Mixture of Models (to name a few), these perception systems can also be used to implement communication protocols, allowing a more natural interaction (gesture, posture, voice) between the robot and the operator [10-12].

The proposed project builds upon the candidate experience in 3D perception [13,14], computer vision [15,16] and augmented reality [17], and unlike previous research that focuses on specific assembly applications using traditional interfaces, the proposed project aims to provide a generic, intuitive and immersive cooperative robot assembly system capable of interacting with the operator using augmented reality and gesture recognition while also learning the necessary assembly skills from demonstration.
